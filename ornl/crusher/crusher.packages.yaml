#
#  packages.yaml
#
#  This file specifies the versions and variants for HPCToolkit's
#  dependency packages and serves as a common reference point for how
#  to build them.  It also specifies the paths or modules for system
#  build tools (cmake, python, etc) to avoid rebuilding them.
#
#  Put this file in one of spack's configuration scope directories:
#    1. <spack-root>/etc/spack -- applies to this spack repo
#    2. <home-dir>/.spack -- applies to any spack on this machine
#           for this user
#    3. any dir -- applies with 'spack -C dir' argument
#
#  There are three sections: (1) regular hpctoolkit dependency specs,
#  (2) GPU and MPI specs, and (3) system build tools: cmake, perl,
#  python and GCC.
#
#  See also:
#    https://spack.readthedocs.io/en/latest/configuration.html
#    https://spack.readthedocs.io/en/latest/build_settings.html
#
packages:

  #------------------------------------------------------------
  # For now, HPCToolkit can only be compiled with GCC.
  # Enforce this requirement here for all Spack packages.
  #------------------------------------------------------------
  all:
    compiler: [gcc@11.2.0]

  #------------------------------------------------------------
  # Always use Cray MPICH to provide MPI on Crusher.
  #------------------------------------------------------------
  mpi:
    buildable: False
    require: "cray-mpich"

  # Note: settings in this file are only preferences, not hard
  # requirements.  If spack chooses a different version or variant,
  # you can force spack to use your version with the new 'require:'
  # field.  Require takes a single, full spec and supersedes both
  # version and variant and should always be quoted.
  #
  # dyninst:
  #   require: "@12.2.0 +openmp"


  #------------------------------------------------------------
  # 1 -- HPCToolkit Prerequisite Packages
  #------------------------------------------------------------


  # This section contains hpctoolkit's prerequisites, recommended
  # versions and necessary variants.  Packages in this section should
  # normally be built from source, even if there is a system install.
  # For most packages, the most recent version will work.
  #
  # Note: the following packages have versions or variants that may
  # need changing for your local system.
  #
  #   1. dyninst -- the latest hpctoolkit release should work with the
  #   latest dyninst release.  But if you're building hpctoolkit at
  #   master or develop, then you may need dyninst master.
  #
  #   2. intel-tbb -- add '~tm' for AMD, Xeon Phi (KNL, etc), and very
  #   old Intel systems that don't support transactional memory.
  #
  # Blue Gene is no longer supported.
  #
  #------------------------------------------------------------


  # hpctoolkit -- these settings can be specified here or else on the
  # spack install command line.
  #
  # Version 'develop' is now the main line of development for
  # recommended versions.
  #
  # +mpi builds hpcprof-mpi (requires mpi).
  # +papi uses papi instead of perfmon (now default).
  # +viewer also installs hpcviewer (default).
  # +cuda, +rocm for GPU support.
  #
  # Note: +mpi is currently disabled for develop.
  #
  hpctoolkit:
    version:  [2022.10.01, develop]


  # hpcviewer -- hpctoolkit databases are platform independent, so the
  # viewer can be installed together with hpctoolkit, or by itself on
  # another machine.
  #
  # Hpctoolkit 2022.10.01 requires hpcviewer >= 2022.10 due to format
  # change in the database.
  #
  # Note: for technical reasons, "2022.10" should be quoted to avoid
  # being misinterpreted as the number 2022.1.  This applies to all
  # 2-field numbers ending with "0".
  #
  hpcviewer:
    version: ["2022.10"]


  # binutils -- used by hpcprof and struct simple (for functions and
  # line map info) and for VMA types in various places.
  #
  # No longer used for latest version or develop.
  #
  # Need >= 2.32 for important bug fixes.
  # Avoid 2.35, it contains a bug that spews errors from hpcprof.
  # Hpctoolkit <= 2020.03.01 requires binutils <= 2.33.1.
  #
  # +libiberty is needed for xmalloc functions in libbfd.
  # ~nls is used to avoid building gettext.
  #
  # binutils:
  #   version:  [2.38]
  #   variants: +libiberty ~nls


  # boost -- used by dyninst and now in hpcstruct.
  #
  # Dyninst requires >= 1.67.
  #   1.72.0 is good for older versions of dyninst.
  #   1.77.0 is good for recent dyninst.
  #   avoid 1.68.0, it breaks the build for hpctoolkit.
  #
  # +atomic, +chrono, etc are for dyninst.
  # +graph, +regex, etc are for toolkit proper.
  # ~debug is to build release (optimized) variant.
  # visibility=global is required for recent hpctoolkit.
  #
  # Note: boost now turns off all libraries by default and expects
  # each package to add what it needs.  So, the best practice is to
  # add everything that we use and not disable anything so as not to
  # interfere with another package's needs.
  #
  boost:
    require: >
      @1.79.0
      +atomic +chrono +date_time +filesystem +system +thread +timer
      +graph +regex +shared +multithreaded ~debug
      ~mpi ~numpy
      visibility=global

  # bzip2 -- used by elfutils to read compressed sections (and copy
  # libs).
  #
  # Bzip2 has resumed development, anything after 1.0.6 is good.
  #
  bzip2:
    version:  [1.0.8]
    variants: +shared


  # dyninst -- used in hpcstruct for line map info, inline sequences
  # and loop analysis.  Hpctoolkit and Dyninst work closely together
  # and Dyninst issues releases infrequently, so sometimes it's
  # necessary to use master for the best results.
  #
  # Latest hpctoolkit needs > 12.0.0.  Several problems were discovered
  # in dyninst 11.0.0 that caused failures in hpcstruct.
  #
  # +openmp is to support openmp threads in parseapi.
  #
  dyninst:
    require:
      - matrix:
        - one_of:
          - '@12.2.1'
          - '@master'
        - '+openmp build_type=RelWithDebInfo'


  # elfutils -- used by dyninst for reading ELF sections.
  #
  # Dyninst requires >= 0.173 and we want a recent version for best
  # support for output from current compilers.
  # 0.184 fixes a bug in binaries compiled with gcc 11.x.
  # dyninst 12.0.1 requires latest elfutils >= 0.186.
  #
  # +bzip2, +xz are for reading compressed sections.
  # ~nls is used to avoid building gettext.
  #
  elfutils:
    version:  [0.186]
    variants: +bzip2 +xz ~nls


  # intel-tbb -- used by dyninst as part of parallel parsing.
  #
  # Dyninst requires >= 2018.6, most anything after that is good.
  #
  # Note: add ~tm for very old x86 or AMD or KNL systems that don't
  # support transactional memory (no-op on non-x86).
  #
  intel-tbb:
    require: "@2020.3 +shared ~tm"


  # intel-xed -- used in various places to parse x86 instructions
  # (used on x86 only).
  #
  # Xed now has releases but changes very slowly.  Normally use the
  # latest release for best results.
  #
  # +debug adds debug symbols (-g).
  # +pic is now required to link with hpcrun
  #
  intel-xed:
    version:  [2022.04.17]
    variants: +debug +pic


  # libdwarf -- used in fnbounds for access to eh frames (not yet
  # available from elfutils).
  #
  # Normally want a recent version for best support for output from
  # current compilers.
  #
  # libdwarf:
  #   version:  [20180129]


  # libiberty -- needed for dyninst configure (although dyninst 10.x
  # doesn't actually link with it).
  #
  # Any recent version is good.
  #
  # +pic is needed to link with dyninst shared library.
  #
  libiberty:
    version:  [2.37]
    variants: +pic


  # libmonitor -- used in hpcrun to provide access to an application's
  # processes and threads.
  #
  # Normally need the most recent snapshot or master.
  #
  # +hpctoolkit is needed for features specific to hpctoolkit (client
  # signals).
  # ~dlopen is needed for 2021.02 and later.
  #
  libmonitor:
    version:  [2022.09.02]
    variants: +hpctoolkit ~dlopen


  # libpfm4 (perfmon) -- used in hpcrun for access to the hardware
  # performance counters.
  #
  # If PAPI is available with the perfmon headers, then that
  # supersedes perfmon.  Otherwise, perfmon provides the perf sample
  # source but not PAPI.
  #
  # Normally want the latest version to support recent CPU types.
  #
  libpfm4:
    version:  [4.11.0]


  # libunwind -- used in hpcrun to help with unwinding.
  #
  # Normally want the most recent release.
  #
  # +xz is for reading compressed symbol tables.
  # +pic is required for linking with hpcrun.
  #
  libunwind:
    version:  [1.6.2]
    variants: +xz +pic


  # mbedtls -- no longer used, this is for old hpctoolkit only
  #
  # Use a recent version >= 2.16.1.
  #
  # +pic is required for linking with hpcrun.
  #
  # mbedtls:
  #   version:  [2.27.0]
  #   variants: +pic


  # papi -- used in hpcrun for access to the hardware performance
  # counters.
  #
  # If there is a pre-built PAPI for the compute nodes, then it may be
  # better to use that and replace this with a 'prefix:' or 'modules:'
  # entry (see the section on cmake).  But make sure that the PAPI
  # include directory contains the perfmon headers, or else you can't
  # use both PAPI and perfmon.
  #
  # If building, then use version >= 5.6.0 for the installed perfmon
  # headers.
  #
  papi:
    externals:
    - spec: papi@6.0.0.15
      modules:
      - papi/6.0.0.15
    buildable: False


  # xerces -- used in hpcprof to read XML.
  #
  # Any recent version is good.
  #
  # transcoder=iconv is needed to avoid linking with libiconv.
  # netaccessor=none is to reduce prereqs.
  #
  xerces-c:
    version:  [3.2.3]
    variants: transcoder=iconv netaccessor=none


  # xz (lzma) -- used by elfutils and libunwind to read compressed
  # sections.
  #
  # Any recent version is good.
  #
  # +pic is required for linking with hpcrun.
  #
  xz:
    version:  [5.2.5]
    variants: +pic


  # yaml-cpp -- now used by develop for writing yaml
  #
  # Use recent version for bug fixes.
  #
  # +shared is required for linking with shared lib
  #
  yaml-cpp:
    version:  [0.7.0]
    variants: +shared build_type=RelWithDebInfo


  # zlib -- used by binutils, elfutils and libdwarf to read compressed
  # sections.
  #
  # Zlib development is stagnant and everything before 1.2.12 is
  # deprecated due to security bugs, so use 1.2.12
  #
  # +optimize is to add -O2.
  #
  zlib:
    version:  [1.2.12]
    variants: +optimize +shared


  #------------------------------------------------------------
  # 2 -- GPU and MPI Packages
  #------------------------------------------------------------

  # Packages in this section can be built from source, but if there is
  # a system install for the compute nodes, then it is normally better
  # to use that.

  # cuda -- nvidia cuda, now in master
  #
  # Requires: 10.x or later, 11.1 preferred. 
  # Note: best to use 11.5 or later, assuming the system supports it
  #
  # cuda:
  #   version: [11.6.2]
  #
  # cuda:
  #   externals:
  #   - spec: cuda@11.6.2
  #     prefix: /path/to/cuda-11.6.2/directory

  #------------------------------------------------------------

  # rocm -- amd gpus, now in master
  #
  # Requires: 4.5.x or later, 5.x is preferred
  #
  # If /opt/rocm is available, then the easiest solution is to use
  # that.  In that case, specify four prereqs (hip, hsa-rocr-dev,
  # roctracer-dev, rocprofiler-dev) as externals in packages.yaml.
  # For example, with /opt/rocm-5.0.0, use:
  #
  # packages:
  hip:
    externals:
    - spec: hip@5.1.0
      prefix: /opt/rocm-5.1.0
      modules: [rocm/5.1.0]
    - spec: hip@5.2.0
      prefix: /opt/rocm-5.2.0
      modules: [rocm/5.2.0]
    - spec: hip@5.3.0
      prefix: /opt/rocm-5.3.0
      modules: [rocm/5.3.0]
    - spec: hip@5.4.0
      prefix: /opt/rocm-5.4.0
      modules: [rocm/5.4.0]
  #
  hsa-rocr-dev:
    externals:
    - spec: hsa-rocr-dev@5.1.0
      prefix: /opt/rocm-5.1.0
      modules: [rocm/5.1.0]
    - spec: hsa-rocr-dev@5.2.0
      prefix: /opt/rocm-5.2.0
      modules: [rocm/5.2.0]
    - spec: hsa-rocr-dev@5.3.0
      prefix: /opt/rocm-5.3.0
      modules: [rocm/5.3.0]
    - spec: hsa-rocr-dev@5.4.0
      prefix: /opt/rocm-5.4.0
      modules: [rocm/5.4.0]
  #
  roctracer-dev:
    externals:
    - spec: roctracer-dev@5.1.0
      prefix: /opt/rocm-5.1.0
      modules: [rocm/5.1.0]
    - spec: roctracer-dev@5.2.0
      prefix: /opt/rocm-5.2.0
      modules: [rocm/5.2.0]
    - spec: roctracer-dev@5.3.0
      prefix: /opt/rocm-5.3.0
      modules: [rocm/5.3.0]
    - spec: roctracer-dev@5.4.0
      prefix: /opt/rocm-5.4.0
      modules: [rocm/5.4.0]
  #
  rocprofiler-dev:
    externals:
    - spec: rocprofiler-dev@5.1.0
      prefix: /opt/rocm-5.1.0
      modules: [rocm/5.1.0]
    - spec: rocprofiler-dev@5.2.0
      prefix: /opt/rocm-5.2.0
      modules: [rocm/5.2.0]
    - spec: rocprofiler-dev@5.3.0
      prefix: /opt/rocm-5.3.0
      modules: [rocm/5.3.0]
    - spec: rocprofiler-dev@5.4.0
      prefix: /opt/rocm-5.4.0
      modules: [rocm/5.4.0]
  #
  llvm-amdgpu:
    externals:
    - spec: llvm-amdgpu@5.1.0
      prefix: /opt/rocm-5.1.0
      modules: [rocm/5.1.0]
    - spec: llvm-amdgpu@5.2.0
      prefix: /opt/rocm-5.2.0
      modules: [rocm/5.2.0]
    - spec: llvm-amdgpu@5.3.0
      prefix: /opt/rocm-5.3.0
      modules: [rocm/5.3.0]
    - spec: llvm-amdgpu@5.4.0
      prefix: /opt/rocm-5.4.0
      modules: [rocm/5.4.0]
  #
  # If ROCM is not already installed, then you can build it as part of
  # hpctoolkit.  In either case, build hpctoolkit with:
  #
  #   spack install hpctoolkit +rocm
  #
  # Note: this is all very fluid and subject to change.

  #------------------------------------------------------------

  # MPI -- hpctoolkit always supports profiling MPI applications.
  # Adding MPI here is only for building hpcprof-mpi, the MPI version
  # of hpcprof.
  #
  # Normally, you want MPI from a module (mpich, mvapich, openmpi)
  # that was built for the correct interconnect system for the compute
  # nodes.  Also, for hpcprof-mpi, MPI should be built with GNU
  # gcc/g++ and the same version used to build hpctoolkit (to keep the
  # C++ libraries in sync).
  #
  # openmpi:
  #   externals:
  #   - spec: openmpi@3.1.6
  #     modules:
  #     - OpenMPI/3.1.6
  #   buildable: False

  cray-mpich:
    buildable: False
    externals:
    - spec: cray-mpich@8.1.17
      modules: [cray-mpich/8.1.17]


  #------------------------------------------------------------
  # 3 -- System Build Tools
  #------------------------------------------------------------

  # Note: Spack can now search for these packages itself.
  # For example:
  #
  #   spack external find cmake
  #
  # This puts an 'externals:' entry for cmake in ~/.spack/packages.yaml
  # if spack finds cmake on your PATH.  This works for cmake, perl,
  # python (and several other packages).  Look for the field
  # 'Externally Detectable' from 'spack info'.

  #------------------------------------------------------------

  # We require cmake >= 3.4, perl >= 5.x and python >= 3.6.
  # There are three ways to satisfy these requirements: a system
  # installed version (eg, /usr), a module or build from scratch.
  #
  # By default, spack will rebuild these, even if your version is
  # perfectly fine.  If you already have an installed version and
  # prefer to use that instead, then uncomment some entry below and
  # edit to fit your system.
  #
  # Note: these three are all build tools.  They are needed to build
  # hpctoolkit, but we don't need them at run time or link with any
  # libraries.  (Not counting hpcsummary which is a python script.)
  # Note:
  #
  #  1. the 'prefix' entry is the parent directory of 'bin', not the
  #  bin directory itself.
  #
  #  2. the 'spec' entry may be a full spec, including dependency specs.
  #
  #  3. a given package may have multiple spec, module and version
  #  entries.
  #
  #  4. 'buildable: False' is optional.  It tells spack that it must use
  #  a 'prefix' or 'modules' entry, or else fail the build.

  #------------------------------------------------------------

  # cmake -- dyninst requires cmake >= 3.4.x.

  # Example for a system install.  This example says that cmake 3.11.0
  # is available at /usr/bin/cmake.
  # cmake:
  #   externals:
  #   - spec: cmake@3.11.0
  #     prefix: /usr
  #   buildable: False

  # Example for a module.  This example says that cmake 3.7.2 is
  # available from module 'CMake/3.7.2'.  You may specify multiple
  # modules, if needed.
  cmake:
    buildable: False
    externals:
    - spec: cmake@3.23.2
      modules: [cmake/3.23.2]


  # Example spec to build cmake from scratch.
  # +ownlibs, ~qt and ~openssl are to reduce prerequisites.
  # cmake:
  #   version:  [3.12.4]
  #   variants: +ownlibs ~qt ~openssl

  #------------------------------------------------------------

  # perl -- autotools requires perl >= 5.x which most systems have.

  # Example for system perl.
  # perl:
  #   externals:
  #   - spec: perl@5.16.3
  #     prefix: /usr
  #   buildable: False

  # Example spec to build perl.
  # perl:
  #   version:  [5.26.2]

  #------------------------------------------------------------

  # python -- we require python >= 3.6

  # Example for system python.  If you have both python2 and python3,
  # then specify them both as 'python' (a special rule for python).
  # python:
  #   externals:
  #   - spec: python@3.6.5
  #     prefix: /usr
  #   - spec: python@2.7.5
  #     prefix: /usr
  #   buildable: False

  # Example for python module.
  python:
    buildable: False
    externals:
    - spec: python@3.9.12.1
      modules: [cray-python/3.9.12.1]


  # Example for autoconf
  autoconf:
    buildable: False
    externals:
    - spec: autoconf@2.69
      modules: [autoconf/2.69]


  # Example for automake
  automake:
    buildable: False
    externals:
    - spec: automake@1.16.3
      modules: [automake/1.16.3]

  # Example spec to build python.
  # ~dbm, etc are to reduce prerequisites.
  # python:
  #   version:  [3.6.5]
  #   variants: ~dbm ~libxml2 ~ssl ~sqlite3 ~tkinter

  #------------------------------------------------------------

  # GCC -- hpctoolkit requires building with GNU gcc/g++.
  # Use version 7.3.x or later.
  #
  # Spack handles compilers specially with the compilers.yaml file.
  # This example is only for building a more recent compiler.
  #
  # gcc:
  #   version: [11.2.0]

