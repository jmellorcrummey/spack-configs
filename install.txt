----------------------------------------------------------------------
To do a spack install and test for a machine <machine> at site <site>:
----------------------------------------------------------------------
1.  Set up a top-level "~/gits/" directory on <machine>, and do all the git clones into it.
	You can put them wherever you want, but the remainder of these
	instructions assume everything goes into ~/gits/

2.  Set up the repositories:
	cd ~/gits
	git clone https://github.com/jmellorcrummey/spack-configs
	    That's where this file lives, so you must have already done that!
        git clone https://github.com/spack/spack.git
	    First remove ~/gits/spack and ~/.spack to set up for a clean install

	The hpctoolkit sources may not be needed, but can be gotten by:
		git clone https://github.com/hpctoolkit/hpctoolkit.git

3. Set up your shell to use spack and the spacklink setup command:
	for bash:
          vi ~/.bashrc	#Add the following:
            umask 022 
            export SPACK_ROOT=~/gits/spack
            export PATH=${SPACK_ROOT}/bin:~gits/spack-configs/bin:${PATH}
            source ${SPACK_ROOT}/share/spack/setup-env.sh
	    export <PROXIES, as needed for spack usage>
	  source ~/.bashrc

4.  Tell spack to use the right gcc:
	module avail gcc
	module load gcc.xxx
            For hptctoolkit, xxx needs to be 4.8 or later, preferably 5.x or later
	  (If you can't find one, tell spack to install one, gcc@7.3.0 is a good choice.
	    To do so, do a spack compiler find, then spack install gcc@7.3.0,
	    Then module load the gcc you've built.)
        spack compiler find  (Again, if you've installed the gcc@7.3.0)

5.  Go to the spack-configs repo, and prepare the appropriate:
	<site>/<machine>/<machine>.config.yaml
	<site>/<machine>/<machine>.modules.yaml
	<site>/<machine>/<machine>.packages.yaml
		files

	The <machine>.config.yaml file should say where to install its packages and modules
	    It seems best to set a top-level directory, <INSTALL> and put the packages in
		  config:
		    install_tree: <INSTALL>/packages
	        and the modules in
		  module_roots:
		    lmod: <INSTALL>/modules/lmod
			etc.
	    The directories and subdirectories used should be created before the install
	    Start with the config.yaml in spack/spack/etc/default or the one for
		in spack-configs for any existing similar <machine>.

	The <machine>.modules.yaml file describes the modules to be built.
	    There are three types of modules, lmod, tcl, and dotkit: the module files in
		the various subdirectories build all three.
		dotkit is now obsolete.
	    The modules built in this step have long, human-hostile names; more
		user-friendly modules are constructed below.

	The <machine>.packages.yaml file describes the dependencies for the install.
	    Start with the one for any existing similar <machine>.
	    You will edit it in step 7, below.
	    Alternatively, you can start with the version in ~/gits/hpctoolkit/spack/packages.yaml

6.  Set up the spack install environment:
	-------------------------------------------------------------------
	It's not clear what spack caches, so to be on the safe side, I remove
	    and re-clone the spack directory, whenever I use a spack that was
	    previously used on a different machine.  I also remove the output
	    packages and modules directories, set in the <machine>.config.yaml.
	-------------------------------------------------------------------
	cd ~/gits
	/bin/rm -rf $SPACK_ROOT
        git clone https://github.com/spack/spack.git
	source ~/.bashrc

	Tell spack how to do the install, by doing
	spacklink <$SPACK_ROOT> <site> <machine>
		spacklink is in the bin directory in the spack-configs repository

	It does the following
		cd $SPACK_ROOT/etc/spack
		rm *.yaml
		ln -s .../spack-configs/<site>/<machine>/<machine>.config.yaml config.yaml
		ln -s .../spack-configs/<site>/<machine>/<machine>.modules.yaml modules.yaml
		ln -s .../spack-configs/<site>/<machine>/<machine>.packages.yaml packages.yaml

	Verify that the machine you are on matches <machine> in the directory and
		the file names of the three symlinks.

	Alternatively, you can copy or link the three files from wherever you choose..

7.  Configure the packages.yaml file, via its symlink, for the system, editing various entries
	a.  Run module avail mpi, and find a suitable MPI to use.
	    Edit the entry for openmpi to refer to it

	b.  Find a suitable cmake on the system or in a module
	    Edit the entry for cmake to refer to it

	c.  Find a suitable perl on the system or in a module
	    Edit the entry for perl to refer to it

	d.  Find a suitable python on the system or in a module
	    Edit the entry for python to refer to it

8.  Decide the specific version and options you want to build.
	hpctoolkit@<version> <options> <compiler>
          <version>
              @2019.08 -- for the 2019.08 release
              @develop -- for current bits
              @gpu -- for GPU versions
          <options>
              +mpi
              +cuda
              +papi
              +all-static
		Multiple options can be concatenated
	  <compiler>
	      %gcc@<version> -- needed if the default is not good enough

	You can see what will get built with:
		spack spec hpctoolkit@<version> <options> <compiler>

9.  Do the real install
	<cmd-to-run> spack install hpctoolkit@<version> <options>

    At LLNL:
	On rzansel and lassen, <cmd-to-run> = "lalloc 1"
	On rzmanta and ray, <cmd-to-run> = "lalloc 1"
	On rzhasgpu and quartz, <cmd-to-run> = "salloc -N 1 -ppdebug"

    At LANL:
	On kodiak, <cmd-to-run> = "salloc -N 1 --qos=interactive"

    At Sandia:
	<cmd-to-run> = "salloc -N1"

    At ANL:

    At NERSC:

10.  Install the hpcviewers, where appropriate
	spack install hpcviewer@<version> <option>
          <version>
              @2019.08 -- for the 2019.08 release

    On x86 <option> is not needed
    On IBM Power machines:
	<option> ="^ibm-java"

11.  Construct a human-readable module to be used by others
	These files can live in a directory,
	    <path-to-modulefiles-directory>/{hpctoolkit,hpcviewer}/
		that is accessible to others.
	The users would run:
	    module use <path-to-modulefiles-directory>
	    module avail hpc
		to see what versions are available

	To construct lmod modules,
	    cd <INSTALL>/modules/lmod
	    find . -name hpctoolkit
		It will be a directory; cd to it
		It will have a *.lua file corresponding to each of the hpctoolkit
		    installs that your ran.
	    cp <version>.lua <path-to-modulefiles-directory>/hpctoolkit/<version>.lua

	The <path-to-modulefiles-directory> may also have a file named "default", which
	    should be a symlink to the version that is the default.
	It also may have a file named "rolling-release", which should be a symlink to
	    the latest version installed.

	Update those two links, as appropriate.

	Follow the same instructions for hpcviewer.


12.  Verify the installation:
	module use <path-to-modulefiles-directory>
	module avail hpc
	For each hpctoolkit <version>:
	    module load <version>
	    hpcrun --version
	Make sure they correspond to the installation as expected


-----------------------------
To run the hpctest test suite
-----------------------------
Be sure you have run "spack compiler find" before starting.
The suite is contains 7 tests, one of which does not compile, and one of which fails in a normal run.
Of the five remaining tests, four require MPI.

1. Clone the test suite repository
	git clone https://github.com/martyitz/hpctest ~/gits/hpctest
	cd ~/gits/hpctest

2.  Initialize: "hpctest init"

3.  Set up the bits to test:
    If you have cloned the repo, its config.yaml will not point to any hpctoolkit
    When you run, the hpctoolkit on your path will be used,
	module avail hpctoolkit
	module load hpctoolkit@xxxx
	    xxxx represents the bits to test

	Or you could edit config.yaml to point to an installation
        profile:
          hpctoolkit:
            path: whatever

5.  Run the full suite:
	<cmd-to-run> hpctest run
	NOTE: The first time you run it, it will take a very long time to do spack install of the sources
	    and the builds for the tests.  After that, it will reuse the builds.

	You can also run a single test, e.g.,
	  <cmd-to-run> hpctest run app/amgmk
	      (That's the only passing test that does not require MPI)

    On machines that are powerful enough to run MPI jobs, no <cmd-to-run> is needed; on login-nodes
    at the various national labs, <cmd-to-run> is needed to run on the compute nodes.

    At LLNL:
	On rzansel and lassen, <cmd-to-run> = "lalloc 1"
	On rzmanta and ray, <cmd-to-run> = "lalloc 1"
	On rzhasgpu and quartz, <cmd-to-run> = "salloc -N 1 -ppdebug"

    At LANL:
	On kodiak, <cmd-to-run> = "salloc -N 1 --qos=interactive"

    At Sandia:
	<cmd-to-run> = "salloc -N1"

    At ANL:

    At NERSC:

To remove previous runs, run "hpctest clean"  It will leave the pre-built executables, but remove all
recorded data.

Each invocation of hpctest creates a directory in .../hpctest/work with a name of the form
    study-<date>--<time>
  where <date> is e.g., "2019-11-25", and <time> is e.g., "15-02-59"
    in this case, referring to the run started at 3:02:59 PM on November 25, 2019.

Under that directory is a set of subdirectories, one for each test.
with names like, e.g.,:
    app--AMG2006:%gcc:-e.REALTIME@10000
each referring to a single test. During the run, hpctest will say:
    running test app/AMG2006
At the end of the test run, result from that test is labelled:
    APP / AMG2006 with %GCC and -e REALTIME@10000
	Note the differing capitalization and punctuation in the various usages.

Each of those individual test subdirectories has a link for the executable, pointing into its
build subdirectory, a build subdirectory which is populated with everything built by
spack for that test.  Each test subdirectory has an OUT subdirectory.

The OUT subdirectory has an OUT.yaml file, summarizing the execution of the test.
It also has various other files and two directories, numbered starting 01-<name>, in
the order with which they were generated during the test.  One of the directories
is "04-hpctoolkit-<app>-measurements" (perhaps with a different number in some runs).
(The <app> here is lower-case, despite the test being upper-case.)
It contains the raw data recorded in the run: "<app>-*.hpcrun" and "<app>-*.hpctrace"
files for each process and thread, and "<app>-*.log" files for each process.

The OUT subdirectory also has a "11-hpctoolkit-<app>-database" (again, perhaps with
a different number in some runs).  It contains the experiment.xml file, and the individual
trace files, copied from the measurements directory.  It also contains a src subdirectory
with what appears to be a copy of the raw source, cloned into a subdirectory tree
mimicing the full path of the subtree of hpctest containing the source.

To look for stack unwind failures, cd to the study-* directory of interest,
and run:
	grep errant */OUT/OUT.yaml

The hpcviewer can be brought up with the name of the "11-hpctoolkit-<app>-database" directory
in the OUT directory for a test.


--------------------
To build hpctoolkit:
--------------------
1.  Get the sources:
	git clone https://github.com/hpctoolkit/hpctoolkit.git hpctoolkit
	cd hpctoolkit
	    Then, [ick the branch you want; for the fnbounds branch:
	git branch fnbounds
	git checkout fnbounds
	git branch --set-upstream-to=origin/fnbounds fnbounds
	git pull	

2.  Construct the build area:
	mkdir BUILD
	cd BUILD

3.  Configure the build
	    (in the BUILD directory)
	../configure --with-spack=<-spack-install-packages-directory> --prefix=<build.out> --enable-develop

4.  Run the build:
	    (in the hpctoolkit-build directory)
	make -j 4 install

If you change any source files, you should only need to rerun the make install

If you want to add or delete source files, or make changes to the build, you need to edit
them into the Makefile.am in the directory where you are making the changes,
and then remove Makefile.in and Makefile, and run "autogen" in the top-level source directory.
Also, if you are editing a *.in file that is used to build a script, you will need to run
"autogen" in the top-level source directory.
Then rerun the make install.
