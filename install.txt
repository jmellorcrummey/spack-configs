To do a spack install and test for a machine <machine> at site <site>:
----------------------------------------------------------------------
1.  Set up a top-level "~/gits/" directory on <machine>, and do all the git clones into it.
	You can put them wherever you want, but the remainder of these
	instructions assume everything goes into ~/gits/

2.  Set up the repositories:
	cd ~/gits
	git clone https://github.com/jmellorcrummey/spack-configs
	    That's where this file lives, so you must have already done that!
        git clone https://github.com/spack/spack.git
	    First remove ~/gits/spack and ~/.spack to set up for a clean install
	git clone https://github.com/martyitz/hpctest

	The hpctoolkit sources are not needed, but can be gotten by:
		git clone https://github.com/hpctoolkit/hpctoolkit.git

3. Set up your shell to use spack:
	for bash:
          vi ~/.bashrc	#Add the following:
            umask 022 
            export SPACK_ROOT=~/gits/spack
            export PATH=${SPACK_ROOT}/bin:${PATH}
            source ${SPACK_ROOT}/share/spack/setup-env.sh
	    export <PROXIES, as needed for spack usage>
	  source ~/.bashrc

4.  Tell spack to use the right gcc:
	module avail gcc
	module load gcc.xxx
            For hptctoolkit, xxx needs to be 4.8 or later, preferably 5.x or later
        spack compiler find

	edit the file ~/.spark/linux/compilers.yaml, to find that gcc.xxx entry, and make
	sure it is for the right architecture.  Add 'gcc/xxxx' in the brackets after module.
		(I don't know why this is important.)

5.  Go to the spack-configs repo, and prepare the appropriate:
	<site>/<machine>/<machine>.config.yaml
	<site>/<machine>/<machine>.modules.yaml
	<site>/<machine>/<machine>.packages.yaml
		files

	The <machine>.config.yaml file should say where to install its packages and modules
		Those directories and subdirectories used should be created before the install
	    Start with the config.yaml in spack/spack/etc/default or the one for
		in spack-configs for any existing similar <machine>.

	The <machine>.modules.yaml file describes the modules to be built.

	The <machine>.packages.yaml file describes the dependencies for the install.
	    Start with the one in ~/gits/hpctoolkit/packages.yaml
		or the one for any existing similar <machine>.
	    You will edit it later.

6.  Set up the spack install environment:
	-------------------------------------------------------------------
	It's not clear what spack caches, so to be on the safe side, I remove
	    and re-clone the spack directory, whenever I use a spack that was
	    previously used on a different machine.  I also remove the output
	    packages and modules directories, set in the <machine>.config.yaml.
	-------------------------------------------------------------------
	cd ~/gits
	/bin/rm -rf $SPACK_ROOT
        git clone https://github.com/spack/spack.git
	source ~/.bashrc

	--------------------------------------------------------------------------
	IT IS EASY TO SCREW THIS NEXT STEP, AND WIND UP WITH A BROKEN INSTALLATION
	--------------------------------------------------------------------------
	cd $SPACK_ROOT/etc/spack
	ln -s .../spack-configs/<site>/<machine>/<machine>.config.yaml config.yaml
	ln -s .../spack-configs/<site>/<machine>/<machine>.modules.yaml modules.yaml
	ln -s .../spack-configs/<site>/<machine>/<machine>.packages.yaml packages.yaml

	There is a script, spacklink, which takes two arguments, <site> and <machine>
	and sets up the links for you.  It is in the top-level spack-configs directory.

	Verify that the machine you are on matches <machine> in the directory and
		the file names of the three symlinks.

7.  Configure the packages.yaml file, via its symlink, for the system, editing various entries
	a.  Run module avail mpi, and find a suitable MPI to use.
	    Edit the entry for openmpi to refer to it

	b.  Find a suitable cmake on the system or in a module
	    Edit the entry for cmake to refer to it

	c.  Find a suitable perl on the system or in a module
	    Edit the entry for perl to refer to it

	d.  Find a suitable python on the system or in a modules
	    Edit the entry for python to refer to it

8.  Decide the specific version and options you want to build.
	hpctoolkit@<version> <options> <compiler>
          <version>
              @2019.08 -- for the 2019.08 release
              @develop -- for current bits
              @gpu -- for GPU versions
          <options>
              +mpi
              +cuda
              +papi
              +all-static
		Multiple options can be concatenated
	  <compiler>
	      %gcc@<version> -- needed if the default is not good enough

	You can see what will get built with:
		spack spec hpctoolkit@<version> <options> <compiler>

9.  Do the real install
	<cmd-to-run> spack install hpctoolkit@<version> <options>

    At LLNL:
	On rzansel and lassen, <cmd-to-run> = "lalloc 1"
	On rzmanta and ray, <cmd-to-run> = "lalloc 1"
	On rzhasgpu and quartz, <cmd-to-run> = "salloc -N 1 -ppdebug"

    At LANL:
	On kodiak, <cmd-to-run> = "salloc -N 1 --qos=interactive"

    At Sandia:
	?

10.  Install the hpcviewers, where appropriate
	spack install hpcviewer@<version> <option>
          <version>
              @2019.07 -- for the 2019.07 release
              @2019.08 -- for the 2019.08 release

    On x86 <option> is not needed
    On IBM Power machines:
	<option> ="^ibm-java"


To run hpctest:
---------------
Be sure you have run "spack compiler find" before starting.

1.  cd ~/gits/hpctest

2.  If you have cloned the repo, its config.yaml will not point to any hpctoolkit
	When you run, the hpctoolkit on your path will be used,
		which seems like the least error-prone methodology

	Or you could edit config.yaml to point to an installation
        profile:
          hpctoolkit:
            path: whatever

3.  Set up the bits to test:
	module avail hpctoolkit
	module load hpctoolkit@xxxx
	    xxxx represents the bits to test

4.  Initialize: "hpctest init"

5.  Run the full suite:
	<cmd-to-run> hpctest run
	    (most tests require MPI launch)
	<cmd-to-run> hpctest run app/amgmk
	    the only passing test that does not.

    At LLNL:
	On rzansel and lassen, <cmd-to-run> = "lalloc 1"
	On rzmanta and ray, <cmd-to-run> = "lalloc 1"
	On rzhasgpu and quartz, <cmd-to-run> = "salloc -N 1 -ppdebug"

    At LANL:
	On kodiak, <cmd-to-run> = "salloc -N 1 --qos=interactive"

    At Sandia:
	?


To build hpctoolkit:
--------------------
1.  Get the sources:
	git clone https://github.com/hpctoolkit/hpctoolkit.git

2.  Run:
	configure --with-spack=<-spack-install-packages-directory> --prefix=<build.out> --enable-develop

3.  Run:
	make -j 4 install
